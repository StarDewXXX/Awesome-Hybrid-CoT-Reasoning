A curated list of research papers on **Hybrid (Adaptive) Chain-of-Thought (CoT) Reasoning**.
## Awesome-Hybrid-CoT-Reasoning ðŸ§ ðŸ¤”



> Hybrid Chain-of-Thought (CoT) is a novel technique designed to accelerate the reasoning process of large reasoning models. It aims to enable the model to dynamically select between Long-CoT and Short-CoT strategies based on the nature of the problem, achieving significantly greater speedup compared to alternative approaches.
---

## ðŸ“Œ LLMs Paper List (sorted by time)

| Title                                                                                                        | Published Date | Link                                         |
| ------------------------------------------------------------------------------------------------------------ | -------------- | -------------------------------------------- |
| [Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)  | 2025-04     | [ðŸ”— arXiv](https://arxiv.org/abs/2504.21659) |
| [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)  | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.10832) |
| [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827) | 2025-05 | [ðŸ”— arXiv](https://arxiv.org/abs/2505.11827) |
| [AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning](https://arxiv.org/abs/2505.11896) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.11896) |
| [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.13379) |
| [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.13417) |
| [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.14183) |
| [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.14631) |
| [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.15154) |
| [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.18822) |
| [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.20101) |
| [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.15612) |

## ðŸ“Œ MLLMs Paper List (sorted by time)
| Title                                                                                                        | Published Date | Link                                         |
| ------------------------------------------------------------------------------------------------------------ | -------------- | -------------------------------------------- |
| [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)  | 2025-04     | [ðŸ”— arXiv](https://arxiv.org/abs/2504.18458) |

## ðŸ“Œ Open source LLMs (sorted by time)
| Title                                                                                                        | Published Date | Link                                         |
| ------------------------------------------------------------------------------------------------------------ | -------------- | -------------------------------------------- |
| [Qwen3 Series (Qwen Team)](https://github.com/QwenLM/Qwen3)  | 2025-04     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.09388) |
| [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431) | 2025-05     | [ðŸ”— arXiv](https://arxiv.org/abs/2505.15431) |



